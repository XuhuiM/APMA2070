{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1024/1*-QTg-_71YF0SVshMEaKZ_g.png\" \n",
    "   width=\"400\" style=\"margin: 50px auto; display: block; position: relative; left: -30px;\" />\n",
    "</div>\n",
    "\n",
    "<!--NAVIGATION-->\n",
    "<!--NAVIGATION-->\n",
    "# [PyTorch Primer](1-pytorch.ipynb) | TensorFlow Primer | [PyT vs TF](3-pytorch_vs_tf.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodule -1.4 : A Primer on PyTorch and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "#### 1. [Installation](#Building-TensorFlow)\n",
    "#### 3. [Basics of TensorFlow](##Basics-of-TensorFlow)\n",
    "#### 4. [Automatic Differentiation](#Automatic-Differentiation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install TensorFlow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors and Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.4963, 0.7682],\n",
      "        [0.0885, 0.1320],\n",
      "        [0.3074, 0.6341]])\n",
      "xsum using mthod1: tensor([1.2645, 0.2205, 0.9415])\n",
      "xsum using mthod2: tensor([1.2645, 0.2205, 0.9415])\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "# Random Matrix of shape=(3,3)\n",
    "x = torch.rand(3,2)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "xsum = torch.sum(x, dim=1)\n",
    "print(f\"xsum using mthod1: {xsum}\")\n",
    "\n",
    "x.sum(dim=1)\n",
    "print(f\"xsum using mthod2: {xsum}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reshape : `view` and  `reshape`  methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inplace Reshaping\n",
    "\n",
    "# A vector of length N=10\n",
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10, 11, 12])\n",
    "# Reshape in amatrix of shape= (2,5)\n",
    "x.view(3,4)\n",
    "\n",
    "# Reshape with unspecified number of rows and 4 column\n",
    "x.view(-1, 4)\n",
    "\n",
    "#### Reshaping via copying\n",
    "\n",
    "# A vector of length N=10\n",
    "x = torch.tensor([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "\n",
    "# Reshape in amatrix of shape= (2,5)\n",
    "y3 = x.reshape(3,4)\n",
    "\n",
    "# Reshape with unspecified number of rows and 4 column\n",
    "y4 = x.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing Norm of a vector using `norm` method\n",
    "\n",
    "$$\n",
    "L_p~\\text{norm}:\n",
    "||{\\mathbf{x}}||_p = \\left(\\sum_i |x_i|^p\\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "L^1~\\text{norm}:\n",
    "|| {\\mathbf{x}} ||_1 = |x_1| + |x_2| + \\ldots + |x_n|\n",
    "$$\n",
    "\n",
    "$$\n",
    "L^2~\\text{norm}:\n",
    "|| {\\mathbf{x}} ||_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Norm of x is:1.3529558181762695\n",
      "L2 Norm of x is:0.9188381433486938\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand(3)\n",
    "x.norm(p=1)\n",
    "x.norm(p=2)\n",
    "print(f\"L1 Norm of x is:{x.norm(p=1)}\")\n",
    "print(f\"L2 Norm of x is:{x.norm(p=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing Norm of a vector without using the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm: is: 1.3529558181762695\n",
      "L2 norm: is: 0.9188381433486938\n",
      "L1 norm: is: 1.3529558181762695\n",
      "L2 norm: is: 0.9188381433486938\n"
     ]
    }
   ],
   "source": [
    "n1 = torch.sum(torch.abs(x))\n",
    "print(f\"L1 norm: is: {n1}\")\n",
    "n2 = torch.sqrt(torch.sum(x**2))\n",
    "print(f\"L2 norm: is: {n2}\")\n",
    "\n",
    "## Or Calling method directly on the data structures\n",
    "n1 = x.abs().sum()\n",
    "print(f\"L1 norm: is: {n1}\")\n",
    "n2 = (x**2).sum().sqrt() \n",
    "print(f\"L2 norm: is: {n2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()  # Check if we can use GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[4,5,8], [3,8,9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping tensors to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 8.],\n",
       "        [3., 8., 9.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_cpu = torch.device(\"cpu\")\n",
    "dev_gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "# Send Tensor to GPU\n",
    "x.to(dev_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 8.],\n",
       "        [3., 8., 9.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the start of your code\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "\n",
    "# For later dispatch\n",
    "x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy ----> PyTorch ----> NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.random((4,4))\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy to PyTorch\n",
    "Y = torch.from_numpy(X)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ---> NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Y.numpy()\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing GPU Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 400, 400])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(100, 400, 400)\n",
    "#B = A.cuda()\n",
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1000])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -n 3 torch.bmm(A, A)\n",
    "%timeit -n 3 torch.bmm(B, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "a = 8\\\\\n",
    "b = 6\\\\\n",
    "c = a + b\\\\\n",
    "d = a * c\\\\\n",
    "$$\n",
    "Compute $ \\frac{\\partial d}{\\partial a}$\n",
    "High School Approach:\n",
    "\n",
    "1. $\\qquad d = a*c$\n",
    "\n",
    "2. $\\qquad  \\frac{\\partial d}{\\partial a} = c * \\frac{\\partial a}{\\partial a} + a * \\frac{\\partial c}{\\partial a}$\n",
    "\n",
    "3. $\\qquad  \\frac{\\partial d}{\\partial a} = c + a* \\frac{\\partial c}{\\partial a}$\n",
    "4. $\\qquad  \\frac{\\partial d}{\\partial a} = (a + b) + a*\\frac{\\partial a}{\\partial a} + a * \\frac{\\partial b}{\\partial a} $\n",
    "5. $\\qquad  \\frac{\\partial d}{\\partial a} = a + b + a*(1 + 0)$\n",
    "6. $\\qquad  \\frac{\\partial d}{\\partial a} = 2a + b $\n",
    "7. $\\qquad  \\frac{\\partial d}{\\partial a} = 2*8+ 6 = 22 $\n",
    "\n",
    "In case if you need to $\\frac{\\partial d}{\\partial a}$, steps 1-7 need to carried out again.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Var:\n",
    "    def __init__(self, val, local_grad=()):\n",
    "        self.val = val\n",
    "        self.local_grad = local_grad\n",
    "         \n",
    "    def __add__(self, other):\n",
    "        y = self.val + other.val\n",
    "        local_grad = ((self, 1), (other, 1))       \n",
    "        return Var(y, local_grad)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        y = self.val*other.val\n",
    "        local_grad = ((self, other.val), (other, self.val))\n",
    "        return Var(y, local_grad)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        y = self.val - other.val\n",
    "        local_grad = ((self, 1), (other, -1))       \n",
    "        return Var(y, local_grad)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_grads(var):\n",
    "    grad = defaultdict(lambda:0)\n",
    "    \n",
    "    def compute_grad(var, path):\n",
    "        for child_var, loc_grad in var.local_grad:\n",
    "            val_path_child = path * loc_grad\n",
    "            grad[child_var] += val_path_child\n",
    "            compute_grad(child_var,val_path_child)   \n",
    "    \n",
    "    compute_grad(var, path=1)\n",
    "    \n",
    "    return grad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AD of addition: 22\n",
      "AD of subtraction: 10\n"
     ]
    }
   ],
   "source": [
    "a = Var(8)\n",
    "b = Var(6)\n",
    "\n",
    "## AD for Addition \n",
    "\n",
    "c = a + b\n",
    "d = a*c\n",
    "\n",
    "grad = get_grads(d)\n",
    "\n",
    "print(f\"AD of addition: {grad[a]}\")\n",
    "\n",
    "## AD for Subtraction \n",
    "\n",
    "c = a - b\n",
    "d = a*c\n",
    "\n",
    "grad = get_grads(d)\n",
    "\n",
    "print(f\"AD of subtraction: {grad[a]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/raj/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def f_x(x, A):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            x (a Tensorflow tensor): the trial eigenvector (i.e. the output\n",
    "                of the neural network)\n",
    "            A (a 2D Numpy array): the matrix to find eigenvectors of\n",
    "        Returns:\n",
    "            f (a Tensorflow tensor): the result of the function f(x(t)), \n",
    "                defined in the paper referenced above.  When x(t) is \n",
    "                converged, f(x(t)) = x(t)\n",
    "        Returns the value of f(x) at a given value of x.\n",
    "    \"\"\"\n",
    "    xTxA = (tf.tensordot(tf.transpose(x), x, axes=1)*A)\n",
    "    # (1- xTAx)*I\n",
    "    xTAxI = (1- tf.tensordot(tf.transpose(x), tf.tensordot(A, x, axes=1), axes=1))*np.eye(matrix_size)\n",
    "    # (xTx*A - (1- xTAx)*I)*x\n",
    "    f = tf.tensordot((xTxA + xTAxI), x, axes=1)\n",
    "\n",
    "    return f\n",
    "\n",
    "##############################\n",
    "# NN EIGENVALUE\n",
    "##############################\n",
    "def NN_Eigenvalue(matrix_size, A, max_iterations, nn_structure, eigen_guess, \n",
    "                    eigen_lr, delta_threshold, verbose):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            matrix_size (an int): the dimension of the matrix\n",
    "            A (a 2D Numpy array): A square, symmetric matrix to find \n",
    "                an eigenvector and eigenvalue of.\n",
    "            max_iterations (an int): the maximum number of training iterations \n",
    "                to be used by the neural network\n",
    "            nn_structure (a list): the number of neurons in each layer of the\n",
    "                neural network\n",
    "            eigen_guess (an int): to find the lowest eigenvalue, a number smaller\n",
    "                than the predicted eigenvalue.  To find the largest eigenvalue,\n",
    "                a number larger than the predicted eigenvalue.\n",
    "            eigen_lr (a float): the learning rate for the portion of the loss\n",
    "                function that controls which eigenvalue is found.  Set to 0.0\n",
    "                to find a random eigenvalue.\n",
    "            delta_threshold (a float): the minimum value desired between two\n",
    "                sequentially calculated eigenvalues\n",
    "            verbose (a boolean): True case prints the state of the eigenvalue for\n",
    "                every 100th training iteration.\n",
    "        Returns:\n",
    "            eigenvalue (a float): the predicted eigenvalue of matrix A\n",
    "            eigenvector (a numpy array): the predicted eigenvector of the matrix \n",
    "                A\n",
    "        Computes a prediction for an eigenvalue and an eigenvector of a given\n",
    "        matrix using a neural network.  Parameters are given to control which\n",
    "        eigenvalue and eigenvector are found.\n",
    "    \"\"\"\n",
    "    # Defining the 6x6 identity matrix\n",
    "    I = np.identity(matrix_size)\n",
    "    \n",
    "    # Create a vector of random numbers and then normalize it\n",
    "    # This is the beginning trial solution eigenvector\n",
    "    x0 = np.random.rand(matrix_size)\n",
    "    x0 = x0/np.sqrt(np.sum(x0*x0))\n",
    "    # Reshape the trial eigenvector into the format for Tensorflow\n",
    "    x0 = np.reshape(x0, (1, matrix_size))\n",
    "\n",
    "    # Convert the above matrix and vector into tensors that can be used by\n",
    "    # Tensorflow\n",
    "    I_tf = tf.convert_to_tensor(I)\n",
    "    x0_tf = tf.convert_to_tensor(x0, dtype=tf.float64)\n",
    "\n",
    "    # Set up the neural network with the specified architecture\n",
    "    with tf.variable_scope('dnn'):\n",
    "        num_hidden_layers = np.size(nn_structure)\n",
    "\n",
    "        # x0 is the input to the neural network\n",
    "        previous_layer = x0_tf\n",
    "        # Hidden layers\n",
    "        for l in range(num_hidden_layers):\n",
    "            current_layer = tf.layers.dense(previous_layer, nn_structure[l],activation=tf.nn.relu )\n",
    "            previous_layer = current_layer\n",
    "\n",
    "        # Output layer\n",
    "        dnn_output = tf.layers.dense(previous_layer, matrix_size)\n",
    "      \n",
    "    # Define the loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        # trial eigenvector is the output of the neural network\n",
    "        x_trial = tf.transpose(dnn_output) \n",
    "        # f(x)\n",
    "        f_trial = tf.transpose(f_x(x_trial, A))\n",
    "        # eigenvalue calculated using the trial eigenvector using the \n",
    "        # Rayleigh quotient formula\n",
    "        eigenvalue_trial = tf.transpose(x_trial)@A@x_trial/(tf.transpose(x_trial)@x_trial)\n",
    "        \n",
    "        x_trial = tf.transpose(x_trial) \n",
    "\n",
    "        # Define the loss function\n",
    "        loss = tf.losses.mean_squared_error(f_trial, x_trial) + \\\n",
    "                eigen_lr*tf.losses.mean_squared_error([[eigen_guess]], eigenvalue_trial)\n",
    "                                                                                                        \n",
    "    # Define the training algorithm and loss function\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    ## Execute the Tensorflow session\n",
    "    with tf.Session() as sess:  \n",
    "        # Initialize the Tensorflow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run()\n",
    "\n",
    "        # Define for calculating the change between successively calculated\n",
    "        # eigenvalues\n",
    "        old_eig = 0\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            sess.run(training_op)\n",
    "            # Compute the eigenvalue using the Rayleigh quotient\n",
    "            eigenvalue = (x_trial.eval() @ (A @ x_trial.eval().T)\n",
    "                        /(x_trial.eval() @ x_trial.eval().T))[0,0]\n",
    "            eigenvector = x_trial.eval()\n",
    "\n",
    "            # Calculate the change between the currently calculated eigenvalue\n",
    "            # and the previous one\n",
    "            delta = np.abs(eigenvalue-old_eig)\n",
    "            old_eig = eigenvalue\n",
    "            \n",
    "            # Print useful information every 100 steps\n",
    "            if verbose:\n",
    "                if i % 100 == 0:\n",
    "                    l = loss.eval()\n",
    "                    print(\"Step:\", i, \"/\",max_iterations, \"loss: \", l,\n",
    "                          \"Eigenvalue:\" , eigenvalue)\n",
    "            # Kill the loop if the change in eigenvalues is less than the \n",
    "            # given threshold\n",
    "            if delta < delta_threshold: \n",
    "                break\n",
    "\n",
    "    # Return the converged eigenvalue and eigenvector\n",
    "    return eigenvalue, eigenvector\n",
    "\n",
    "\n",
    "def random_symmetric (matrix_size):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            matrix_size (an int): the size of the matrix to be constructed\n",
    "        Returns:\n",
    "            A (a 2D Numpy array): a symmetric matrix\n",
    "        Constructs a symmetric matrix of the given size filled with random numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a matrix filled with random numbers\n",
    "    A = np.random.rand (matrix_size, matrix_size)\n",
    "\n",
    "    # Ensure that matrix A is symmetric\n",
    "    A = (np.transpose(A) + A) / 2\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raj/opt/anaconda3/lib/python3.8/site-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/Users/raj/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 / 5000 loss:  48.65819 Eigenvalue: 0.24466103886133028\n",
      "Step: 100 / 5000 loss:  44.49423 Eigenvalue: 3.2960032212392214\n",
      "Step: 200 / 5000 loss:  44.494217 Eigenvalue: 3.2960097205956993\n",
      "Step: 300 / 5000 loss:  44.494217 Eigenvalue: 3.2960097207113894\n",
      "\n",
      " Numpy Eigenvalues: \n",
      " [ 3.29600972 -0.9247771   0.14777154 -0.43953424 -0.2140425   0.64863175]\n",
      "\n",
      " Final Numerical Eigenvalue \n",
      " 3.2960097207113903\n",
      "\n",
      "\n",
      "Absolute difference between Numerical Eigenvalue and TensorFlow DNN =  7.549516567451064e-15\n"
     ]
    }
   ],
   "source": [
    "# Defining variables\n",
    "matrix_size = 6 # Size of the matrix\n",
    "max_iterations = 5000 # Maximum number of iterations\n",
    "nn_structure = [100,100] # Number of hidden neurons in each layer\n",
    "eigen_guess =  70 # Guess for the eigenvalue (see the header of NN_Eigenvalue)\n",
    "eigen_lr = 0.01 # Eigenvalue learnign rate (see the header of NN_Eigenvalue)\n",
    "delta_threshold = 1e-16 # Kill condition\n",
    "verbose = True # True to display state of neural network evrey 100th iteration\n",
    "\n",
    "# Create the matrix to be used\n",
    "A = random_symmetric (matrix_size)\n",
    "\n",
    "\n",
    "# Find the eigenvalues and the eigenvectors using Numpy to compare to the \n",
    "numpy_eigenvalues, numpy_eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# Reset the Tensorflow graph, causes an error if this is not here\n",
    "# Since the above cells are not re-ran every time this one is, they are not \n",
    "# reset.  This line is needed to reset the Tensorflow computational graph to\n",
    "# avoid variable redefinition errors. \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Calcualte the estimate of the eigenvalue and the eigenvector\n",
    "eigenvalue, eigenvector = NN_Eigenvalue(matrix_size, A, max_iterations,\n",
    "                                        nn_structure, eigen_guess, eigen_lr, \n",
    "                                        delta_threshold, verbose)\n",
    "\n",
    "## Compare with the analytical solution\n",
    "print(\"\\n Numpy Eigenvalues: \\n\", numpy_eigenvalues)\n",
    "print(\"\\n Final Numerical Eigenvalue \\n\", eigenvalue)\n",
    "diff = np.min(abs(numpy_eigenvalues - eigenvalue))\n",
    "print(\"\\n\")\n",
    "print('Absolute difference between Numerical Eigenvalue and TensorFlow DNN = ',diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
